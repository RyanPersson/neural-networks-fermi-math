{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import format_float_scientific\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints large floats in readable format.\n",
    "def sci(number):\n",
    "    print('{:0.3e}'.format(number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synapse Firings as FLOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brain is a massive computational substrate that performs countless computations per second.\n",
    "Many people have speculated about the raw computational power of the brain in terms of actual bit calculations. But I believe a better analogy, would be computation in terms of FLOPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because of the way we run digital neural networks.\n",
    "\n",
    "We run large amounts of matrix computations in which float vector *inputs* are multiplied by *weight* matrices of floats, to produce a vector *output* of floats, which is then ran through an *activation function*.\n",
    "\n",
    "Each input element in a vector, being multiplied by a weight in the matrix, is analagous to an impulse firing into a synapse, from a neuron.\n",
    "Each element in the input & output vectors, can be thought of as individual neurons, and each weight in the matrix, is the connection between one input neuron and another output neuron. So each floating point multiplication of an input, by a weight, is analogous to the firing of a synapse from one neuron to another. \n",
    "\n",
    "This allows us to actually compare quantitatively, the computational power of biological and digital networks, if we weigh a synapse firing, as the ONE floating point operation which simulates/represents it in a digital neural network.\n",
    "\n",
    "#### This idea leads to some interesting math below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ~85 billion *neurons_per_brain*, $( \\frac{neurons}{brain} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.500e+10\n"
     ]
    }
   ],
   "source": [
    "neurons_per_brain = 85*1e9\n",
    "sci(neurons_per_brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ~7000 *synapses_per_neuron* $ (\\frac{synapse}{neuron}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapses_per_neuron = 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of synapses *synapses_per_brain* is therefore 85 billion x 7000 = 5.95e14 synapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.950e+14\n"
     ]
    }
   ],
   "source": [
    "synapses_per_brain = neurons_per_brain*synapses_per_neuron\n",
    "sci(synapses_per_brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [the best research I've found](https://aiimpacts.org/rate-of-neuron-firing/#:~:text='But%20generally%2C%20the%20range%20for,'),\n",
    "\n",
    "\n",
    "\n",
    "The **average** firing rate per neuron is between .3 & 1.8 firings per second\n",
    "\n",
    "The commonly cited figures for neuron firing rate are ~200Hz but these are considered to be outdated/innacurate, because these are measurements of INDIVIDUAL neurons in highly active parts of the brain.\n",
    "\n",
    "The majority of neurons are *likely much quieter* than this (see footnote). for global calculation on brain performace, we want the *average* firing rate per neuron.\n",
    "This is calculated in the above link, by brain energy consumption per volume, neuron density & the [metabolic cost](https://www.nature.com/articles/s41598-019-43460-8) of each individual neuron firing.\n",
    "\n",
    "$$ .3Hz < FiringRate < 1.8Hz $$\n",
    "\n",
    "This variable is set here as 1Hz & can be updated when better info is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(footnote from above) The AVERAGE firing rate throughout the brain, is likely much lower than the firing rate of neurons in highly active areas, because much of the brain is most likely long term memory & information that isn't constantly being accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_rate = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then make the approximation, that a *Synapse* between two neurons, is equivalent to a weight in a digital neural network,\n",
    "then a single synapse performs ONE floating point operation per firing of it's parent neuron.\n",
    "\n",
    "Therefore, the FLOPS (floating point operations per second) rate of a neuron is ~7000 $\\frac{operations}{second}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the global, computational power of a human brain, as   *neurons_per_brain* x *synapses_per_neuron* x *firing_rate* $\\frac{operations}{second}$ or FLOPS\n",
    "\n",
    "as 5.95e14 or 595 TeraFlops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.950e+14\n"
     ]
    }
   ],
   "source": [
    "brain_flops = neurons_per_brain*synapses_per_neuron*firing_rate\n",
    "sci(brain_flops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the human brain has a computational power of \n",
    "\n",
    "*number of neurons* X *synapses per neuron* X *average firing rate* = $ 5.95x10^{14} $ FLOPS Or\n",
    "595 TFLOPS (teraflops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, the NVIDIA GeForce 3090 (A new, modern GPU) has a theoretical performance of 35.6 TFLOPS (or\n",
    "3.56e13 Flops) in float32 computations.\n",
    "\n",
    "& 69 TFLOPS in float16 computations. [source](https://www.tomshardware.com/news/nvidia-geforce-rtx-3090-ga102-everything-we-know)\n",
    "\n",
    "I have no idea, how *precise* a firing synapse in a neuron is, and if so, what precision it could be analogous to in terms of bits. #ResearchThis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.560e+13\n"
     ]
    }
   ],
   "source": [
    "sci(35.6*1e12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF all the above calculations are true, then computational power is actually approuching that of the human brain.\n",
    "\n",
    "The reason the human brain is still more powerful, is likely it's MASSIVE (possibly *maximal*) Parralelization. Due to being a physical neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF we can build better algorithms, & implement parallelization on a MASSIVE scale, we can build more intelligent software NOW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brain is a sparse, VERY slow (1Hz) MASSIVELY distributed computing system. Could probably be better be thought of,\n",
    "as 85 Billion Networked computers, each operating at 1Hz with 7000 compuational (cores?)\n",
    "\n",
    "Massive distributed nework of 85 billion 7000 FLOP computers LOL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The computers we build in sillicon are performing calculations at speeds like ~3GigaHz (3e9 Hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the ~1Hz firing rate of neurons in the brain, this is insane.\n",
    "The brain is still more powerful because 85 billion massively parrallel computers is nothing to sneeze at, even if they are small and slow. The massively parrallel, asyncronous nature of that many computing \"cores\" is the reason that their slow speed ends up not mattering that much in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**BUT**\n",
    "WE can potentially do the same thing with sillicon computers, which are operating at 3GHz & sending signals at close to the speed of light, vs the human brains 1Hz, 200 mph signal propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we're close to there in terms of raw computational. What are the roadblocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's look at the actual memory storage in the synapses of the human brain:\n",
    "\n",
    "A synapse in a bioligical neural network is analagous to a weight (between neurons) in a digital neural network.\n",
    "\n",
    "A single layer in a neural network, is represnted as a Matrix tranform, usually followed by an activation function.\n",
    "Neural network computation is simulated by multiplying an input vector (representing the set of input neurons), then multiplying this vector by the matrix of weights. Each weight, multiplied by an element of the input vector, then summed as part of one of the output vectors, represents the impulse from one neuron to another. Therefore, that single, floating point operation, of multiplying an element in the input vector by a weight, represents a synapse firing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output vector is then ran through an activation function which attempts to map it to an output in the next layer in a sensible way.\n",
    "\n",
    "The compuationally **hard** part of this, is the actual matrix computation, because it is $ O(n^2) $ space complexity to store a matrix representing weights from n input, to n output neurons. & the time complexity of multiplying the input vector by the matrix is also $O(n^2)$. This can be parrallelized, but still is a bottleneck. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this method of representing a neural network digitally, information is stored in the float values of the weights. So the theoretical, ACTUAL amount of information in a human neural network, is $ 5.95x10^{14} $ synapse weights or floats.\n",
    "\n",
    "It's unclear how accurate each of these floats needs to be, as the actualy physical property they represent, is (#double check this) the ease with which a signal propagates through the synapse. This is likely a messy biological process (voltage front, propagating through a partially insulated tube of ionized liquid), & not precise in practice. So a lower accuracy float is probably sufficient to simulate it accurately. \n",
    "\n",
    "\n",
    "For the sake of getting a ballpark figure, we'll say that a weight can be accurately represented by a 32 bit float, then the amount of data in a brain is 4bytes x $ 5.95x10^{14} $ = 2380 TB of floats. This is a MASSIVE amount of data, and too much to store in RAM needed to run matrix computations.\n",
    "\n",
    "HOWEVER, \n",
    "The majority of this data is not accessed often/i.e. the neurons fire very rarely and the data does not need to be quickly accessible.\n",
    "\n",
    "The more important parts of the brain (i.e. regions of neurons which ARE firing at ~200Hz) will nescessarily be less than the 2380TB total, though how much less is a big unknown.\n",
    "\n",
    "The sets of weights representing THESE regions in a brain, would need to be held in some sort of RAM like memory, in order to quickly pass weights to GPU's in order to crunch large numbers of floats.\n",
    "\n",
    "The addtional ~2380 TB of rarely accessed data could be stored in some sort of distributed database which pulled in sections of memory as needed.\n",
    "\n",
    "---\n",
    "\n",
    "This is all an extremely hard technical challenge.\n",
    "\n",
    "But if my math is right (& please double check it, let me know if you see issues or am wrong), we're ACTUALLY THERE in terms of raw computational power.\n",
    "\n",
    "600 FLOPS is doable with 4 or 5 modern, high performance GPUs. The remaining roadblock is no longer computational power, but information retrieval/distributed massive databases, LARGE amounts of RAM, and structuring it all in a form that actually mimics the complex architecture of the human brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
